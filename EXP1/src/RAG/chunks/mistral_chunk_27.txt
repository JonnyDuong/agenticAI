5.4
Multilingual benchmarks

To evaluate Magistral’s multilingual capabilities, we interacted with Magistral Medium in multiple
languages to check that it could reason and answer in the user’s language. We also tested Magistral
Medium on multilingual (French, Spanish, German, Italian, Russian, and Chinese) versions of the
AIME 2024 benchmark. These multilingual versions were created by translating the questions from
English into each of the languages. The results are presented in Table 4. We see that the model
performs 4.3-9.9% lower on multilingual versions compared to English, which corresponds to 1-3
questions on the actual AIME test, possibly because we constrained the language of reasoning. This
degradation is roughly similar to that of the base model. Note that on the multilingual benchmarks,
all of the reasoning and the final response are conducted in the input language (i.e., not English).

9


--- Page 10 ---
Table 3: Performance of Magistral Small compared with different training setups across various bench-
marks. We report the performance of three distinct 24B models: Mistral Small 24B fine-tuned on reasoning
traces from Magistral Medium (SFT), Mistral Small 24B trained from scratch with RL (RL only), and Mistral
Small 24B fine-tuned on Magistral Medium traces and subsequently enhanced with RL (SFT + RL) which is the
final Magistral Small. We observe that the combination of fine-tuning on reasoning traces with RL leads to the
best performance. For the evaluation of Humanity’s Last Exam, only the text subset was considered.

Task
SFT
RL-only
SFT + RL (Magistral Small)

AIME’24pass@1
65.4
65.8